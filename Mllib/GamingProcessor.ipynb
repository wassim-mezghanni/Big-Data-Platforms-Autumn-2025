{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you turn this problem in, make sure everything runs as expected. First, **restart the kernel** (in the menubar, select Kernel$\\rightarrow$Restart) and then **run all cells** (in the menubar, select Cell$\\rightarrow$Run All).\n",
    "\n",
    "Make sure you fill in any place that says `YOUR CODE HERE` or \"YOUR ANSWER HERE\". You can run all the tests with the validate button. If the validate command takes too long, you can also confirm that you pass all the tests if you can run through the whole notebook without getting validation errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6bd80a0883591efded4260a5991bd57e",
     "grade": false,
     "grade_id": "cell-c9581b6ad17b53d9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "For this problem set, we'll be using the Jupyter notebook:\n",
    "\n",
    "![](jupyter.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2c5c0ea232af7dcfe11b8cb4f8a99495",
     "grade": false,
     "grade_id": "cell-1b0c57d4df18a1e3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Mllib exercises\n",
    "\n",
    "In this notebook you will implement multiple small methods that are used to predict if a customer is going to be a `paid_customer`. We use logistic regression (https://en.wikipedia.org/wiki/Logistic_regression) for solving the classification problem.\n",
    "\n",
    "We will use a sample of data from http://cs.hut.fi/u/arasalo1/resources/osge_pool-1-thread-1.data.zip.\n",
    "\n",
    "Your task is to create a machine learning pipeline that transform the data so that mllib's logistic regression can make predictions on if a customer is going to pay for the service. First method `convert` transforms the file into a dataframe so that it is easier to process. Categorical features are transformed using method `indexer`. `featureAssembler` creates a single feature vector. Most mllib's machine learning algorithms require this. `scaler` scales the variables. `createModel`creates and trains the logistic regression model. Training data has been transformed properly using the previous methods. Finally, `predict` is used to make predictions whether the user is a paying customer using the trained model.\n",
    "\n",
    "Note: try to avoid additional imports as it can cause problems with server tests.\n",
    "\n",
    "### Data schema\n",
    "\n",
    "| column_header | type |description |\n",
    "| :------------- | :--- | :----------- |\n",
    "| cid | uuid | customer id |\n",
    "| cname | string | name of the user |\n",
    "| email | string | email address of the user |\n",
    "| gender | string | customer's gender |\n",
    "| age | int | age of the customer |\n",
    "| address | string | user provided address during registration, stores only US based addresses other countries gets 'N/A' |\n",
    "| country | string | country to which customer belongs to |\n",
    "| register_date | long | date on which user registered with us in milliseconds |\n",
    "| friend_count | int | number of friends a user has |\n",
    "| lifetime | int | number of days a user has been active since registration date |\n",
    "| citygame_played | int | number of times citygame has been played by user |\n",
    "| pictionarygame_played | int | number of times pictionary game has been played by user |\n",
    "| scramblegame_played | int | number of times scaramble game has been played by user |\n",
    "| snipergame_played | int | number of times sniper game has been played by user |\n",
    "| revenue | int | revenue generated by the user |\n",
    "| paid_subscriber | string | whether the customer is paid customer or not, represented by `yes` or `no` |\n",
    "\n",
    "\n",
    "Use Spark machine learning library mllib's Binomial Logistic Regression algorithm.  \n",
    "https://spark.apache.org/docs/latest/ml-classification-regression.html#binomial-logistic-regression\n",
    "\n",
    "Use these features for training your model:\n",
    "* gender \n",
    "* age\n",
    "* country\n",
    "* friend_count\n",
    "* lifetime\n",
    "* citygame_played\n",
    "* pictionarygame_played\n",
    "* scramblegame_played\n",
    "* snipergame_played\n",
    "* paid_subscriber(this is the feature to predict)\n",
    "\n",
    "The data contains categorical features, so you need to change them accordingly.  \n",
    "https://spark.apache.org/docs/latest/ml-features.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a9f0f89b51595d354cc8ac7d92d79395",
     "grade": false,
     "grade_id": "cell-0f850f2cc2f99366",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import *\n",
    "from pyspark.ml.feature import *\n",
    "from pyspark.ml.classification import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "spark = SparkSession.builder\\\n",
    "    .master(\"local\")\\\n",
    "    .appName(\"gaming\")\\\n",
    "    .config(\"spark.dynamicAllocation.enabled\", \"true\")\\\n",
    "    .config(\"spark.shuffle.service.enabled\", \"true\")\\\n",
    "    .getOrCreate()\n",
    "\n",
    "sampleDataPath = \"testData.data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "57c990210ca3df112d8d715e7d39fb6b",
     "grade": false,
     "grade_id": "cell-a27d59b341ba3a5c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#Generate random sample\n",
    "import random\n",
    "\n",
    "randomData = \"randomsample.data\"\n",
    "\n",
    "with open(sampleDataPath) as sampleFile:\n",
    "    lines = random.sample(sampleFile.readlines(), 4000)\n",
    "\n",
    "outF = open(randomData, \"w\")\n",
    "outF.writelines(lines)\n",
    "outF.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0b03034f1da4fbdc1471eedf4f8a8dd7",
     "grade": false,
     "grade_id": "cell-d62a1aad5888f54b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Convert\n",
    "`convert` creates a dataframe, removes unnecessary colums and converts the rest to right format.   \n",
    "Data schema:\n",
    "* gender: Double (1 if male else 0)\n",
    "* age: Double\n",
    "* country: String\n",
    "* friend_count: Double\n",
    "* lifetime: Double\n",
    "* game1: Double (citygame_played)\n",
    "* game2: Double (pictionarygame_played)\n",
    "* game3: Double (scramblegame_played)\n",
    "* game4: Double (snipergame_played)\n",
    "* paid_customer: Double (1 if yes else 0)  \n",
    "\n",
    "The function already creates a SQL table called \"gaming\", your job is to remove unneccesary columns and convert the rest to right format. Hint: SQL `SELECT` query and `CAST`. You will also need to use `IF` to properly parse and read some of the variables. e.g. `IF(gender='male',1,0)`.\n",
    "\n",
    "param `path`: path to file  \n",
    "`return`: converted DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ab0486c9dea6471e8b45c2e96f2568c4",
     "grade": false,
     "grade_id": "cell-0f036589fc66950b",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def convert(path):\n",
    "    originalCols = StructType([\\\n",
    "    StructField(\"session_id\", StringType(),False),\\\n",
    "    StructField(\"cname\", StringType(),False),\\\n",
    "    StructField(\"email\",StringType(),False),\\\n",
    "    StructField(\"gender\",StringType(),False),\\\n",
    "    StructField(\"age\",DoubleType(),False),\\\n",
    "    StructField(\"address\",StringType(),False),\\\n",
    "    StructField(\"country\",StringType(),True),\\\n",
    "    StructField(\"register_date\",StringType(),False),\\\n",
    "    StructField(\"friend_count\",DoubleType(),False),\\\n",
    "    StructField(\"lifetime\",DoubleType(),False),\\\n",
    "    StructField(\"game1\",DoubleType(),False),\\\n",
    "    StructField(\"game2\",DoubleType(),False),\\\n",
    "    StructField(\"game3\",DoubleType(),False),\\\n",
    "    StructField(\"game4\",DoubleType(),False),\\\n",
    "    StructField(\"revenue\",DoubleType(),False),\\\n",
    "    StructField(\"paid_customer\",StringType(),False)])\n",
    "    data = spark.read.option(\"header\",\"false\").schema(originalCols).csv(path)\n",
    "    data.createOrReplaceTempView(\"gaming\")\n",
    "    # Select only needed columns in the required order and cast/convert types\n",
    "    converted = data.selectExpr(\n",
    "        \"IF(gender='male', 1.0, 0.0) as gender\",\n",
    "        \"CAST(age as double) as age\",\n",
    "        \"CAST(country as string) as country\",\n",
    "        \"CAST(friend_count as double) as friend_count\",\n",
    "        \"CAST(lifetime as double) as lifetime\",\n",
    "        \"CAST(game1 as double) as game1\",\n",
    "        \"CAST(game2 as double) as game2\",\n",
    "        \"CAST(game3 as double) as game3\",\n",
    "        \"CAST(game4 as double) as game4\",\n",
    "        \"CAST(IF(paid_customer='yes', 1.0, 0.0) as double) as paid_customer\"\n",
    "    )\n",
    "    return converted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2ceeaec6438fdc3973382f57a6bed13e",
     "grade": false,
     "grade_id": "cell-c5fe0802b37f6d1c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "data = convert(sampleDataPath)\n",
    "data.cache()\n",
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ca92a0d8e3cdd302cc188f1ed99be75f",
     "grade": true,
     "grade_id": "cell-adf9a5c07d840635",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "'''convert tests'''\n",
    "correctCols = StructType([\\\n",
    "StructField(\"gender\",DoubleType(),False),\\\n",
    "StructField(\"age\",DoubleType(),True),\\\n",
    "StructField(\"country\",StringType(),True),\\\n",
    "StructField(\"friend_count\",DoubleType(),True),\\\n",
    "StructField(\"lifetime\",DoubleType(),True),\\\n",
    "StructField(\"game1\",DoubleType(),True),\\\n",
    "StructField(\"game2\",DoubleType(),True),\\\n",
    "StructField(\"game3\",DoubleType(),True),\\\n",
    "StructField(\"game4\",DoubleType(),True),\\\n",
    "StructField(\"paid_customer\",DoubleType(),False)])\n",
    "\n",
    "fakeData = [(0.0,1.0,\"A\",1.0,1.0,1.0,1.0,1.0,1.0,0.0)]\n",
    "\n",
    "fakeDf = spark.createDataFrame(fakeData, correctCols)\n",
    "\n",
    "assert data.dtypes == fakeDf.dtypes, \"the schema was expected to be %s but it was %s\" % (fakeDf.dtypes, data.dtypes)\n",
    "\n",
    "test1 = str(data.sample(False, 0.01, seed=12345).limit(1).first())\n",
    "correct1 = \"Row(gender=1.0, age=20.0, country='UK', friend_count=2.0, lifetime=4.0, game1=0.0, game2=0.0, game3=0.0, game4=4.0, paid_customer=0.0)\"\n",
    "assert test1 == correct1, \"the row was expected to be %s but it was %s\" % (correct1, test1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9d9efd0f2d7b3c9c90c3e1e53f1f8b1f",
     "grade": false,
     "grade_id": "cell-0a958c2ac19966e9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Indexer\n",
    "`indexer` converts categorical features into doubles.  \n",
    "https://spark.apache.org/docs/latest/ml-features.html#stringindexer  \n",
    "`country` is the only categorical feature.  \n",
    "After these modifications schema should be:\n",
    "  * gender: Double (1 if male else 0)\n",
    "  * age: Double\n",
    "  * country: String\n",
    "  * friend_count: Double\n",
    "  * lifetime: Double\n",
    "  * game1: Double (citygame_played)\n",
    "  * game2: Double (pictionarygame_played)\n",
    "  * game3: Double (scramblegame_played)\n",
    "  * game4: Double (snipergame_played)\n",
    "  * paid_customer: Double (1 if yes else 0)\n",
    "  * country_index: Double\n",
    "  \n",
    "param `df`: DataFrame  \n",
    "`return`: transformed Dataframe. The returned dataframe should have a new column called \"country_index\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4b9edd4a131d93cbec033dcaf5117b13",
     "grade": false,
     "grade_id": "cell-2bacebf519b88b19",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def indexer(df):\n",
    "    indexer = StringIndexer(inputCol='country', outputCol='country_index')\n",
    "    model = indexer.fit(df)\n",
    "    return model.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bfd1d3b51ffb333a0d5989c12947af28",
     "grade": false,
     "grade_id": "cell-67800749e92168d7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "indexed = indexer(data)\n",
    "indexed.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cad0eb29ea5e1718b2a6f3d64e912f7b",
     "grade": true,
     "grade_id": "cell-ee803468a162e783",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "'''indexer tests'''\n",
    "correctCols = StructType([\\\n",
    "StructField(\"gender\",DoubleType(),False),\\\n",
    "StructField(\"age\",DoubleType(),False),\\\n",
    "StructField(\"country\",StringType(),True),\\\n",
    "StructField(\"friend_count\",DoubleType(),False),\\\n",
    "StructField(\"lifetime\",DoubleType(),False),\\\n",
    "StructField(\"game1\",DoubleType(),False),\\\n",
    "StructField(\"game2\",DoubleType(),False),\\\n",
    "StructField(\"game3\",DoubleType(),False),\\\n",
    "StructField(\"game4\",DoubleType(),False),\\\n",
    "StructField(\"paid_customer\",DoubleType(),False),\\\n",
    "StructField(\"country_index\",DoubleType(),False)])\n",
    "\n",
    "fakeData = [(0.0,1.0,\"A\",1.0,1.0,1.0,1.0,1.0,1.0,0.0,0.0)]\n",
    "\n",
    "fakeDf = spark.createDataFrame(fakeData, correctCols)\n",
    "\n",
    "assert indexed.dtypes == fakeDf.dtypes, \"the schema was expected to be %s but it was %s\" % (fakeDf.dtypes, indexed.dtypes)\n",
    "\n",
    "test2 = str(indexed.sample(False, 0.01, seed=12345).limit(1).first())\n",
    "correct2 = \"Row(gender=1.0, age=20.0, country='UK', friend_count=2.0, lifetime=4.0, game1=0.0, game2=0.0, game3=0.0, game4=4.0, paid_customer=0.0, country_index=1.0)\"\n",
    "assert test2 == correct2, \"the row was expected to be %s but it was %s\" % (correct2, test2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6f32c0330d80068b2e71ca400c4fba79",
     "grade": false,
     "grade_id": "cell-cf5d55641055bd63",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Feature Assembler\n",
    "`featureAssembler` combines features into one vector. Most mllib algorithms require this step.  \n",
    "https://spark.apache.org/docs/latest/ml-features.html#vectorassembler  \n",
    "In this task your vector assembler should take and combine the following columns in the same order listed:  \n",
    "```[\"gender\", \"age\",\"friend_count\",\"lifetime\",\"game1\",\"game2\",\"game3\",\"game4\",\"country_index\"]```.\n",
    "\n",
    "param `df`: Dataframe that is transformed using indexer  \n",
    "`return` transformed Dataframe. The returned dataframe should have a new column called \"features\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7a64089df33960fc9596c3180a60c73e",
     "grade": false,
     "grade_id": "cell-6ad1d748f9da8a8a",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def featureAssembler(df):\n",
    "    assembler = VectorAssembler(inputCols=[\n",
    "        'gender', 'age', 'friend_count', 'lifetime', 'game1', 'game2', 'game3', 'game4', 'country_index'\n",
    "    ], outputCol='features')\n",
    "    return assembler.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "adc6b61bd8cceeafc5ce8ecf1ff7aef2",
     "grade": false,
     "grade_id": "cell-66f4b261a69b9cd9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assembled = featureAssembler(indexed)\n",
    "assembled.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "559ef0cb83574567917cb396bc19d12a",
     "grade": true,
     "grade_id": "cell-72527054a3bfbb12",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "'''assembler schema test'''\n",
    "from pyspark.ml.linalg import *\n",
    "from pyspark.ml.linalg import VectorUDT\n",
    "\n",
    "correctCols = StructType([\\\n",
    "StructField(\"gender\",DoubleType(),False),\\\n",
    "StructField(\"age\",DoubleType(),False),\\\n",
    "StructField(\"country\",StringType(),True),\\\n",
    "StructField(\"friend_count\",DoubleType(),False),\\\n",
    "StructField(\"lifetime\",DoubleType(),False),\\\n",
    "StructField(\"game1\",DoubleType(),False),\\\n",
    "StructField(\"game2\",DoubleType(),False),\\\n",
    "StructField(\"game3\",DoubleType(),False),\\\n",
    "StructField(\"game4\",DoubleType(),False),\\\n",
    "StructField(\"paid_customer\",DoubleType(),False),\\\n",
    "StructField(\"country_index\",DoubleType(),False),\\\n",
    "StructField(\"features\", VectorUDT(),True)])\n",
    "\n",
    "fakeData = [(0.0,1.0,\"A\",1.0,1.0,1.0,1.0,1.0,1.0,0.0,0.0,(Vectors.dense([1.0, 2.0])))]\n",
    "\n",
    "fakeDf = spark.createDataFrame(fakeData, correctCols)\n",
    "\n",
    "assert assembled.dtypes == fakeDf.dtypes, \"the schema was expected to be %s but it was %s\" % (fakeDf.dtypes, assembled.dtypes)\n",
    "\n",
    "test3 = str(assembled.sample(False, 0.01, seed=12345).limit(1).first())\n",
    "correct3 = \"Row(gender=1.0, age=20.0, country='UK', friend_count=2.0, lifetime=4.0, game1=0.0, game2=0.0, game3=0.0, game4=4.0, paid_customer=0.0, country_index=1.0, features=DenseVector([1.0, 20.0, 2.0, 4.0, 0.0, 0.0, 0.0, 4.0, 1.0]))\"\n",
    "assert test3 == correct3, \"the row was expected to be %s but it was %s\" % (correct3, test3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "97917f83fd493732474d0950432b4271",
     "grade": false,
     "grade_id": "cell-c1ce008f100710a3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Scaler\n",
    "`scaler` standardizes data to improve performance.  \n",
    "https://spark.apache.org/docs/latest/ml-features.html#standardscaler  \n",
    "For this task please remember to set the `withStd` and `withMean` parameters to true.\n",
    "\n",
    "param `df` Dataframe that is transformed using featureAssembler  \n",
    "param `outputColName` name of the scaled feature vector (output column name)  \n",
    "`return` transformed Dataframe. The returned dataframe should have a new column named after the passed `outputColName` parameter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "94b02cdfdfd4c7d526deca26d8d717ba",
     "grade": false,
     "grade_id": "cell-0f30c6a6fc8aaaba",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def scaler(df, outputColName):\n",
    "    sc = StandardScaler(inputCol='features', outputCol=outputColName, withStd=True, withMean=True)\n",
    "    model = sc.fit(df)\n",
    "    return model.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2cc7343e668d30bea402a6c2585cb64e",
     "grade": false,
     "grade_id": "cell-4fab9f80e65d3803",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "scaled = scaler(assembled, \"scaledFeatures\")\n",
    "scaled.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0aded1cd3159f742d0e91f298335563e",
     "grade": true,
     "grade_id": "cell-a182434c9e8c0d9c",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "'''scaler schema test'''\n",
    "correctCols = StructType([\\\n",
    "StructField(\"gender\",DoubleType(),False),\\\n",
    "StructField(\"age\",DoubleType(),False),\\\n",
    "StructField(\"country\",StringType(),True),\\\n",
    "StructField(\"friend_count\",DoubleType(),False),\\\n",
    "StructField(\"lifetime\",DoubleType(),False),\\\n",
    "StructField(\"game1\",DoubleType(),False),\\\n",
    "StructField(\"game2\",DoubleType(),False),\\\n",
    "StructField(\"game3\",DoubleType(),False),\\\n",
    "StructField(\"game4\",DoubleType(),False),\\\n",
    "StructField(\"paid_customer\",DoubleType(),False),\\\n",
    "StructField(\"country_index\",DoubleType(),False),\\\n",
    "StructField(\"features\", VectorUDT(),True),\\\n",
    "StructField(\"scaledFeatures\", VectorUDT(),True)])\n",
    "\n",
    "fakeData = [(0.0,1.0,\"A\",1.0,1.0,1.0,1.0,1.0,1.0,0.0,0.0,(Vectors.dense([1.0, 2.0])),(Vectors.dense([2.0, 0.0])))]\n",
    "\n",
    "fakeDf = spark.createDataFrame(fakeData, correctCols)\n",
    "\n",
    "assert scaled.dtypes == fakeDf.dtypes, \"the schema was expected to be %s but it was %s\" % (fakeDf.dtypes, scaled.dtypes)\n",
    "\n",
    "test4 = str(scaled.sample(False, 0.01, seed=12345).limit(1).first())\n",
    "correct4 = \"Row(gender=1.0, age=20.0, country='UK', friend_count=2.0, lifetime=4.0, game1=0.0, game2=0.0, game3=0.0, game4=4.0, paid_customer=0.0, country_index=1.0, features=DenseVector([1.0, 20.0, 2.0, 4.0, 0.0, 0.0, 0.0, 4.0, 1.0]), scaledFeatures=DenseVector([0.9008, -0.6236, -0.5183, -0.6848, -0.5844, -0.6369, -0.7638, -0.3154, -0.1343]))\"\n",
    "assert test4 == correct4, \"the row was expected to be %s but it was %s\" % (correct4, test4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "dcebd2fc5755042bd160d02e53c622fb",
     "grade": false,
     "grade_id": "cell-6f316c74cc34b8db",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Create Model\n",
    "`createModel` creates a Logistic Regression model. When training, 5 iterations should be enough.  \n",
    "https://spark.apache.org/docs/latest/ml-classification-regression.html#binomial-logistic-regression\n",
    "\n",
    "param `training` transformed dataframe  \n",
    "param `featuresCol` name of the features column  \n",
    "param `labelCol` name of the label col (paid_customer)  \n",
    "param `predCol` name of the prediction col  \n",
    "`return` trained Logistic Regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c5b6862d42761350edf2930f992c4e1e",
     "grade": false,
     "grade_id": "cell-c8d2049a75f2e440",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def createModel(training, featuresCol, labelCol, predCol):\n",
    "    lr = LogisticRegression(maxIter=5, featuresCol=featuresCol, labelCol=labelCol, predictionCol=predCol)\n",
    "    model = lr.fit(training)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "526088245c3c0f3edce714f37d8acef4",
     "grade": false,
     "grade_id": "cell-25896bc1cf4ef868",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#split the dataset into training(70%) and prediction(30%) sets\n",
    "splitted = scaled.randomSplit([0.7,0.3])\n",
    "\n",
    "model = createModel(splitted[0],\"scaledFeatures\",\"paid_customer\",\"prediction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e2299dd1c42b7a99d76d0313987edc6e",
     "grade": false,
     "grade_id": "cell-8c4c41dcf8dd0844",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Predict\n",
    "Given a transformed and normalized dataset `predict` predicts if the customer is going to subscribe to the service.\n",
    "\n",
    "85% correct will give you 3 points (all tests pass).  \n",
    "70% correct will give you 2 points.  \n",
    "50% correct will give you 1 point.\n",
    "\n",
    "param `model` trained logistic regression model  \n",
    "param `dataToPredict` normalized dataframe for prediction  \n",
    "`return` DataFrame with predicted scores (1.0 == yes, 0.0 == no)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5f523c69e48c1e43d2a68e0acfa30cc7",
     "grade": false,
     "grade_id": "cell-0f428aa47138be07",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def predict(model, dataToPredict):\n",
    "    return model.transform(dataToPredict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cae46b3425cf6af4483f33ed7955eb60",
     "grade": false,
     "grade_id": "cell-3941a4e7923325ea",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "predictions = predict(model, splitted[1])\n",
    "correct = predictions.where(\"prediction == paid_customer\").count()\n",
    "total = predictions.count()\n",
    "print((correct / total) * 100, \"% predicted correctly\")\n",
    "predictions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "660fe4619907f303caa5cbf27faee47d",
     "grade": true,
     "grade_id": "cell-9c03a0af4edc3a65",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "'''prediction correctness test'''\n",
    "data = convert(randomData)\n",
    "data.cache()\n",
    "indexed = indexer(data)\n",
    "assembled = featureAssembler(indexed)\n",
    "scaled = scaler(assembled, \"scaledFeatures\")\n",
    "splitted = scaled.randomSplit([0.7,0.3])\n",
    "model = createModel(splitted[0],\"scaledFeatures\",\"paid_customer\",\"prediction\")\n",
    "predictions = predict(model, splitted[1])\n",
    "correct = predictions.where(\"prediction == paid_customer\").count()\n",
    "total = predictions.count()\n",
    "answer = (correct / total) * 100\n",
    "print(answer, \"% predicted correctly\")\n",
    "assert answer >= 50.0, \"less than 50% predicted correctly, you get 0 points\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9b24745d48b0be8e1558bb1fd7c48cd6",
     "grade": true,
     "grade_id": "cell-9f0f5384922238d8",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert answer >= 70.0, \"less than 70% predicted correctly, you get 1 point\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e6f44b1235f0c857558491b1700c639c",
     "grade": true,
     "grade_id": "cell-2d042a867e9496e3",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert answer >= 85.0, \"less than 85% predicted correctly, you get 2 points\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cef00ef05113460baea465dae1e22e4b",
     "grade": false,
     "grade_id": "cell-8acd6dcad7102812",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
